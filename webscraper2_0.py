# -*- coding: utf-8 -*-
"""webscraper2.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iXD63aJxav4ZzvXfv9AadtOAXMoUis5U
"""

import requests
from bs4 import BeautifulSoup
import time
import random
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service

#initialize webdriver
options = webdriver.ChromeOptions()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')

driver = webdriver.Chrome(options=options)

#open main page
URL = "https://en.falundafa.org/announcements.html"
driver.get(URL)
time.sleep(1)

def extract_announce():
  soup = BeautifulSoup(driver.page_source, 'html5lib')
  announce = soup.find_all('div', class_='content')

  for posts in announce:
    # Extract title from <strong>
    title_tag = posts.find('strong')
    title = title_tag.text.strip() if title_tag else 'No Title'

    # Extract paragraphs, excluding empty ones or those with only &nbsp;
    paragraphs = posts.find_all('p')
    filtered_paragraphs = []

    for p in paragraphs:
        text = p.get_text().replace('\xa0', '').strip()
        if text:  # Ensure we are not including empty paragraphs
            # Check if paragraph contains a link
            link_tag = p.find('a', href=True)
            if link_tag:
                link = f"https://en.falundafa.org/{link_tag['href']}"
                text = f"{text} ({link})"  # Append the link to the content
            filtered_paragraphs.append(text)

    # Print extracted data
    print("Content:")
    for para in filtered_paragraphs:
        print(f"- {para}")

    print("-" * 50)  # Separator between announcements

extract_announce()

#navigate to the books page
try:
  menu = driver.find_element(By.ID, 'menu')
  target_link = menu.find_element(By.XPATH, '//a[contains(@href, "falun-dafa-books.html?v=bks04")]')
  time.sleep(1)
  driver.execute_script("arguments[0].click();", target_link)
  time.sleep(1)
  print("Navigated to Books and Recent Writings Page")
  print("Current URL", driver.current_url)
except Exception as e:
  print("NA", e)

#navigate to recent writings section of same page
try:
  recent_writings = "https://en.falundafa.org/falun-dafa-recent-writings.html"
  driver.get(recent_writings)
  print("Now in Recent Writings Section")
  print("Current URL", driver.current_url)
except Exception as e:
  print("Link Not Found", e)

def extract_recent():
    try:
        #target specific location of the elements
        target_link = driver.find_elements(By.XPATH, '//div[@id="list-of-recent-writings"]//div[@class="row-book "]/a/div/div[@class="box span-title"]')[0]
        #get the text directly
        book = target_link.text.strip()
        print(book)
    except Exception as e:
        print("Error extracting book title:", e)

extract_recent()
